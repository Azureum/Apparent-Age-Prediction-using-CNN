{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This code is imported from the following project: https://github.com/asmith26/wide_resnets_keras\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, add, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "sys.setrecursionlimit(2 ** 20)\n",
    "np.random.seed(2 ** 10)\n",
    "\n",
    "\n",
    "class WideResNet:\n",
    "    def __init__(self, image_size, depth=16, k=8):\n",
    "        self._depth = depth\n",
    "        self._k = k\n",
    "        self._dropout_probability = 0\n",
    "        self._weight_decay = 0.0005\n",
    "        self._use_bias = False\n",
    "        self._weight_init = \"he_normal\"\n",
    "\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            logging.debug(\"image_dim_ordering = 'th'\")\n",
    "            self._channel_axis = 1\n",
    "            self._input_shape = (3, image_size, image_size)\n",
    "        else:\n",
    "            logging.debug(\"image_dim_ordering = 'tf'\")\n",
    "            self._channel_axis = -1\n",
    "            self._input_shape = (image_size, image_size, 3)\n",
    "\n",
    "    # Wide residual network http://arxiv.org/abs/1605.07146\n",
    "    def _wide_basic(self, n_input_plane, n_output_plane, stride):\n",
    "        def f(net):\n",
    "            # format of conv_params:\n",
    "            #               [ [kernel_size=(\"kernel width\", \"kernel height\"),\n",
    "            #               strides=\"(stride_vertical,stride_horizontal)\",\n",
    "            #               padding=\"same\" or \"valid\"] ]\n",
    "            # B(3,3): orignal <<basic>> block\n",
    "            conv_params = [[3, 3, stride, \"same\"],\n",
    "                           [3, 3, (1, 1), \"same\"]]\n",
    "\n",
    "            n_bottleneck_plane = n_output_plane\n",
    "\n",
    "            # Residual block\n",
    "            for i, v in enumerate(conv_params):\n",
    "                if i == 0:\n",
    "                    if n_input_plane != n_output_plane:\n",
    "                        net = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        net = Activation(\"relu\")(net)\n",
    "                        convs = net\n",
    "                    else:\n",
    "                        convs = BatchNormalization(axis=self._channel_axis)(net)\n",
    "                        convs = Activation(\"relu\")(convs)\n",
    "\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "                else:\n",
    "                    convs = BatchNormalization(axis=self._channel_axis)(convs)\n",
    "                    convs = Activation(\"relu\")(convs)\n",
    "                    if self._dropout_probability > 0:\n",
    "                        convs = Dropout(self._dropout_probability)(convs)\n",
    "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
    "                                          strides=v[2],\n",
    "                                          padding=v[3],\n",
    "                                          kernel_initializer=self._weight_init,\n",
    "                                          kernel_regularizer=l2(self._weight_decay),\n",
    "                                          use_bias=self._use_bias)(convs)\n",
    "\n",
    "            # Shortcut Connection: identity function or 1x1 convolutional\n",
    "            #  (depends on difference between input & output shape - this\n",
    "            #   corresponds to whether we are using the first block in each\n",
    "            #   group; see _layer() ).\n",
    "            if n_input_plane != n_output_plane:\n",
    "                shortcut = Conv2D(n_output_plane, kernel_size=(1, 1),\n",
    "                                         strides=stride,\n",
    "                                         padding=\"same\",\n",
    "                                         kernel_initializer=self._weight_init,\n",
    "                                         kernel_regularizer=l2(self._weight_decay),\n",
    "                                         use_bias=self._use_bias)(net)\n",
    "            else:\n",
    "                shortcut = net\n",
    "\n",
    "            return add([convs, shortcut])\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "    # \"Stacking Residual Units on the same stage\"\n",
    "    def _layer(self, block, n_input_plane, n_output_plane, count, stride):\n",
    "        def f(net):\n",
    "            net = block(n_input_plane, n_output_plane, stride)(net)\n",
    "            for i in range(2, int(count + 1)):\n",
    "                net = block(n_output_plane, n_output_plane, stride=(1, 1))(net)\n",
    "            return net\n",
    "\n",
    "        return f\n",
    "\n",
    "#    def create_model(self):\n",
    "    def __call__(self):\n",
    "        logging.debug(\"Creating model...\")\n",
    "\n",
    "        assert ((self._depth - 4) % 6 == 0)\n",
    "        n = (self._depth - 4) / 6\n",
    "\n",
    "        inputs = Input(shape=self._input_shape)\n",
    "\n",
    "        n_stages = [16, 16 * self._k, 32 * self._k, 64 * self._k]\n",
    "\n",
    "        conv1 = Conv2D(filters=n_stages[0], kernel_size=(3, 3),\n",
    "                              strides=(1, 1),\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=self._weight_init,\n",
    "                              kernel_regularizer=l2(self._weight_decay),\n",
    "                              use_bias=self._use_bias)(inputs)  # \"One conv at the beginning (spatial size: 32x32)\"\n",
    "\n",
    "        # Add wide residual blocks\n",
    "        block_fn = self._wide_basic\n",
    "        conv2 = self._layer(block_fn, n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1, 1))(conv1)\n",
    "        conv3 = self._layer(block_fn, n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2, 2))(conv2)\n",
    "        conv4 = self._layer(block_fn, n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2, 2))(conv3)\n",
    "        batch_norm = BatchNormalization(axis=self._channel_axis)(conv4)\n",
    "        relu = Activation(\"relu\")(batch_norm)\n",
    "\n",
    "        # Classifier block\n",
    "        pool = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\")(relu)\n",
    "        flatten = Flatten()(pool)\n",
    "        predictions_g = Dense(units=2, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "        predictions_a = Dense(units=101, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
    "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\")(flatten)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=[predictions_g, predictions_a])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "\n",
    "def main():\n",
    "    depth = 16\n",
    "    k = 8\n",
    "    weight_file = \"weights.18-4.06.hdf5\"\n",
    "\n",
    "    if not weight_file:\n",
    "        weight_file = \"weights.18-4.06.hdf5\"\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img_size = 64\n",
    "    model = WideResNet(img_size, depth=depth, k=k)()\n",
    "    model.load_weights(weight_file)\n",
    "    model.summary()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"error: failed to capture image\")\n",
    "            return -1\n",
    "\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "        detected = detector(input_img, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - 0.4 * w), 0)\n",
    "            yw1 = max(int(y1 - 0.4 * h), 0)\n",
    "            xw2 = min(int(x2 + 0.4 * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + 0.4 * h), img_h - 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            faces[i,:,:,:] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "        if len(detected) > 0:\n",
    "            results = model.predict(faces)\n",
    "            ages=np.zeros(101)\n",
    "            for i in range(101):\n",
    "                ages[i]=i\n",
    "            top3=(sorted(zip(results[1][0],ages), reverse=True)[:3])\n",
    "            print (top3)\n",
    "            answer_age=top3[0][1]\n",
    "            predicted_genders = results[0]\n",
    "            \n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}\".format(int(answer_age),\n",
    "                                    \"F\" if predicted_genders[i][0] > 0.5 else \"M\")\n",
    "            draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"result\", img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147456      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  2048        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 128)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147456      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  147456      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 128)  0           conv2d_6[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  294912      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 256)  589824      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 256)  32768       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 256)  0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  589824      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  589824      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 256)  0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 256)  1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  1179648     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 512)  2359296     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 512)  131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 512)  0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 512)  2359296     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 512)  2359296     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 512)  0           conv2d_16[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 512)  2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 512)  0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 131072)       0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            262144      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 101)          13238272    flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,463,856\n",
      "Trainable params: 24,456,656\n",
      "Non-trainable params: 7,200\n",
      "__________________________________________________________________________________________________\n",
      "[(0.042377602, 47.0), (0.042344231, 42.0), (0.037926044, 45.0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.035359189, 31.0), (0.035138518, 32.0), (0.033026475, 27.0)]\n",
      "[(0.047232173, 42.0), (0.044981692, 45.0), (0.04233573, 39.0)]\n",
      "[(0.051797267, 32.0), (0.046436936, 38.0), (0.046407297, 42.0)]\n",
      "[(0.061621986, 32.0), (0.061520241, 30.0), (0.061508641, 31.0)]\n",
      "[(0.044466808, 42.0), (0.03736762, 32.0), (0.03657534, 45.0)]\n",
      "[(0.052510329, 42.0), (0.051184494, 39.0), (0.04482504, 38.0)]\n",
      "[(0.054384992, 32.0), (0.049005669, 36.0), (0.048848961, 30.0)]\n",
      "[(0.056551915, 32.0), (0.05486789, 30.0), (0.044060428, 31.0)]\n",
      "[(0.040022258, 31.0), (0.03910042, 32.0), (0.036832742, 26.0)]\n",
      "[(0.031444337, 42.0), (0.030412067, 32.0), (0.028390918, 31.0)]\n",
      "[(0.058551833, 42.0), (0.058064789, 45.0), (0.046910558, 38.0)]\n",
      "[(0.029392891, 24.0), (0.029131196, 32.0), (0.027881363, 26.0)]\n",
      "[(0.071766242, 45.0), (0.061861601, 42.0), (0.056988329, 31.0)]\n",
      "[(0.057946559, 45.0), (0.055234466, 31.0), (0.054025784, 42.0)]\n",
      "[(0.050586496, 32.0), (0.040936276, 38.0), (0.039096933, 30.0)]\n",
      "[(0.054864384, 32.0), (0.043681309, 35.0), (0.043356609, 31.0)]\n",
      "[(0.068319507, 45.0), (0.066799991, 38.0), (0.065367498, 42.0)]\n",
      "[(0.11097708, 30.0), (0.10393643, 31.0), (0.084873743, 32.0)]\n",
      "[(0.082018204, 30.0), (0.073325314, 31.0), (0.072783493, 32.0)]\n",
      "[(0.082925461, 32.0), (0.076559894, 31.0), (0.067971215, 30.0)]\n",
      "[(0.073362008, 42.0), (0.056610912, 39.0), (0.055884022, 38.0)]\n",
      "[(0.062951431, 42.0), (0.051506922, 32.0), (0.044063807, 45.0)]\n",
      "[(0.060283292, 27.0), (0.057913825, 31.0), (0.056192983, 32.0)]\n",
      "[(0.06722191, 31.0), (0.062363483, 32.0), (0.055933662, 30.0)]\n",
      "[(0.063978158, 32.0), (0.057779472, 36.0), (0.054310776, 31.0)]\n",
      "[(0.050347481, 32.0), (0.046472341, 31.0), (0.04160013, 27.0)]\n",
      "[(0.058249667, 32.0), (0.052022211, 36.0), (0.051049616, 31.0)]\n",
      "[(0.059067436, 32.0), (0.054955058, 31.0), (0.054034811, 36.0)]\n",
      "[(0.060015015, 32.0), (0.055781722, 31.0), (0.051421523, 30.0)]\n",
      "[(0.058048241, 31.0), (0.057379972, 27.0), (0.052997246, 26.0)]\n",
      "[(0.04638141, 42.0), (0.046120256, 32.0), (0.042612236, 45.0)]\n",
      "[(0.060820743, 32.0), (0.050808452, 36.0), (0.050174329, 35.0)]\n",
      "[(0.048905984, 42.0), (0.046586465, 32.0), (0.045550037, 45.0)]\n",
      "[(0.061877053, 42.0), (0.058549907, 45.0), (0.041644458, 39.0)]\n",
      "[(0.055441637, 32.0), (0.052306641, 31.0), (0.045495991, 30.0)]\n",
      "[(0.052757926, 32.0), (0.043727733, 27.0), (0.042982731, 31.0)]\n",
      "[(0.053598054, 42.0), (0.049007237, 45.0), (0.047386352, 32.0)]\n",
      "[(0.043207955, 51.0), (0.042757012, 47.0), (0.042446587, 45.0)]\n",
      "[(0.057135809, 45.0), (0.052934378, 47.0), (0.050619557, 42.0)]\n",
      "[(0.059288476, 45.0), (0.053706214, 42.0), (0.041676398, 32.0)]\n",
      "[(0.054462444, 32.0), (0.049870804, 42.0), (0.047093607, 36.0)]\n",
      "[(0.056553867, 42.0), (0.053920262, 32.0), (0.051634923, 45.0)]\n",
      "[(0.060184162, 45.0), (0.055579681, 42.0), (0.048039224, 47.0)]\n",
      "[(0.058348175, 32.0), (0.049553987, 31.0), (0.04415549, 36.0)]\n",
      "[(0.065951683, 31.0), (0.05945516, 32.0), (0.051506724, 30.0)]\n",
      "[(0.064677909, 32.0), (0.064595893, 31.0), (0.054867312, 36.0)]\n",
      "[(0.06099876, 42.0), (0.054183334, 45.0), (0.04747856, 39.0)]\n",
      "[(0.059108939, 42.0), (0.052462906, 32.0), (0.05034944, 36.0)]\n",
      "[(0.068328075, 32.0), (0.067107536, 42.0), (0.060281146, 37.0)]\n",
      "[(0.05610377, 32.0), (0.050934233, 31.0), (0.048208985, 42.0)]\n",
      "[(0.060487919, 31.0), (0.057360724, 32.0), (0.051926032, 38.0)]\n",
      "[(0.057649866, 32.0), (0.05320999, 31.0), (0.049213096, 30.0)]\n",
      "[(0.085896567, 42.0), (0.079377003, 39.0), (0.069629453, 37.0)]\n",
      "[(0.073997527, 45.0), (0.069729917, 42.0), (0.045232896, 38.0)]\n",
      "[(0.090906955, 42.0), (0.079059556, 45.0), (0.067130737, 39.0)]\n",
      "[(0.10228759, 42.0), (0.084702641, 45.0), (0.063602977, 38.0)]\n",
      "[(0.12981647, 42.0), (0.09556146, 45.0), (0.065540366, 39.0)]\n",
      "[(0.065353364, 42.0), (0.057702448, 45.0), (0.05734399, 39.0)]\n",
      "[(0.06412036, 42.0), (0.059544016, 39.0), (0.055531543, 45.0)]\n",
      "[(0.075793177, 42.0), (0.062517837, 45.0), (0.056081541, 38.0)]\n",
      "[(0.12010649, 42.0), (0.084415898, 45.0), (0.057490434, 38.0)]\n",
      "[(0.095449887, 42.0), (0.067906775, 37.0), (0.066336632, 45.0)]\n",
      "[(0.076521657, 37.0), (0.071046434, 42.0), (0.064103678, 39.0)]\n",
      "[(0.063617185, 32.0), (0.062347479, 31.0), (0.055474743, 30.0)]\n",
      "[(0.074522384, 42.0), (0.068311244, 37.0), (0.063297793, 39.0)]\n",
      "[(0.060844071, 27.0), (0.059499074, 31.0), (0.051660474, 26.0)]\n",
      "[(0.068253592, 31.0), (0.057644125, 30.0), (0.055798449, 32.0)]\n",
      "[(0.076669566, 42.0), (0.070933931, 37.0), (0.059476081, 32.0)]\n",
      "[(0.084324487, 42.0), (0.066708036, 37.0), (0.064237341, 39.0)]\n",
      "[(0.072611503, 27.0), (0.06892927, 31.0), (0.065985486, 26.0)]\n",
      "[(0.062669612, 42.0), (0.057122789, 32.0), (0.055254824, 36.0)]\n",
      "[(0.076691695, 31.0), (0.061715152, 35.0), (0.059913676, 32.0)]\n",
      "[(0.065600023, 31.0), (0.065050952, 32.0), (0.062436227, 35.0)]\n",
      "[(0.081663191, 31.0), (0.066074714, 30.0), (0.064568937, 32.0)]\n",
      "[(0.0673884, 31.0), (0.062537096, 32.0), (0.055766046, 35.0)]\n",
      "[(0.065515362, 42.0), (0.054552596, 45.0), (0.051670965, 39.0)]\n",
      "[(0.066711895, 31.0), (0.063432224, 32.0), (0.061342258, 36.0)]\n",
      "[(0.070184164, 45.0), (0.066796511, 42.0), (0.045676515, 39.0)]\n",
      "[(0.064790532, 45.0), (0.057380732, 42.0), (0.048373446, 32.0)]\n",
      "[(0.061417792, 31.0), (0.057349589, 32.0), (0.050577033, 35.0)]\n",
      "[(0.071854986, 32.0), (0.062176339, 31.0), (0.054235689, 35.0)]\n",
      "[(0.060424808, 45.0), (0.05550855, 42.0), (0.045127809, 32.0)]\n",
      "[(0.079033501, 45.0), (0.076202132, 42.0), (0.052573904, 39.0)]\n",
      "[(0.078225791, 42.0), (0.066528596, 39.0), (0.062865451, 45.0)]\n",
      "[(0.078455284, 32.0), (0.078439012, 31.0), (0.063411236, 30.0)]\n",
      "[(0.071730971, 32.0), (0.065958455, 36.0), (0.065265216, 35.0)]\n",
      "[(0.093585551, 38.0), (0.064597927, 35.0), (0.060990665, 39.0)]\n",
      "[(0.064036131, 32.0), (0.060699873, 38.0), (0.059630603, 42.0)]\n",
      "[(0.064284049, 42.0), (0.061702475, 39.0), (0.051982656, 38.0)]\n",
      "[(0.084548295, 45.0), (0.051659226, 42.0), (0.04847819, 37.0)]\n",
      "[(0.068322174, 45.0), (0.065598056, 42.0), (0.043047149, 38.0)]\n",
      "[(0.077877693, 45.0), (0.077456281, 42.0), (0.060731702, 39.0)]\n",
      "[(0.047791623, 31.0), (0.044927031, 38.0), (0.044576183, 39.0)]\n",
      "[(0.07616552, 45.0), (0.047385, 42.0), (0.045721531, 47.0)]\n",
      "[(0.041549768, 45.0), (0.041207764, 31.0), (0.03848641, 32.0)]\n",
      "[(0.082986794, 45.0), (0.082502559, 42.0), (0.05427416, 38.0)]\n",
      "[(0.08802703, 42.0), (0.087860547, 45.0), (0.049956914, 38.0)]\n",
      "[(0.1109293, 42.0), (0.099523894, 45.0), (0.052812375, 38.0)]\n",
      "[(0.086916059, 45.0), (0.05239464, 42.0), (0.050896972, 47.0)]\n",
      "[(0.05406398, 45.0), (0.041157819, 42.0), (0.039342791, 31.0)]\n",
      "[(0.078819402, 31.0), (0.060960915, 32.0), (0.053720679, 30.0)]\n",
      "[(0.066774406, 45.0), (0.044144817, 43.0), (0.043501526, 42.0)]\n",
      "[(0.058629587, 45.0), (0.048318338, 52.0), (0.047447924, 47.0)]\n",
      "[(0.067570731, 31.0), (0.064029157, 35.0), (0.063753523, 38.0)]\n",
      "[(0.083047099, 35.0), (0.078155711, 32.0), (0.067329362, 31.0)]\n",
      "[(0.093910255, 31.0), (0.07809367, 32.0), (0.059324868, 35.0)]\n",
      "[(0.054413721, 42.0), (0.046193946, 38.0), (0.044909447, 45.0)]\n",
      "[(0.069640458, 31.0), (0.065503374, 35.0), (0.064222977, 32.0)]\n",
      "[(0.056379959, 31.0), (0.054966863, 32.0), (0.048463814, 42.0)]\n",
      "[(0.075582005, 31.0), (0.072740667, 32.0), (0.070994698, 35.0)]\n",
      "[(0.077807382, 31.0), (0.076590903, 32.0), (0.066261806, 35.0)]\n",
      "[(0.063329175, 31.0), (0.060557254, 32.0), (0.050775506, 30.0)]\n",
      "[(0.07403978, 31.0), (0.059408158, 32.0), (0.057460692, 30.0)]\n",
      "[(0.050700482, 32.0), (0.045691181, 31.0), (0.043163788, 42.0)]\n",
      "[(0.05132826, 32.0), (0.049322322, 27.0), (0.04556004, 24.0)]\n",
      "[(0.06164971, 31.0), (0.061012145, 32.0), (0.054003261, 30.0)]\n",
      "[(0.057380114, 31.0), (0.056589328, 32.0), (0.051593937, 38.0)]\n",
      "[(0.055894356, 31.0), (0.053372964, 32.0), (0.048182216, 38.0)]\n",
      "[(0.062950052, 31.0), (0.062197588, 32.0), (0.052640446, 30.0)]\n",
      "[(0.048376277, 32.0), (0.045511916, 31.0), (0.045416918, 27.0)]\n",
      "[(0.043159965, 32.0), (0.042147692, 42.0), (0.041073762, 45.0)]\n",
      "[(0.041525435, 32.0), (0.035798725, 42.0), (0.03484669, 35.0)]\n",
      "[(0.056586668, 37.0), (0.05579121, 31.0), (0.055657011, 32.0)]\n",
      "[(0.038522471, 32.0), (0.038437683, 42.0), (0.036119055, 35.0)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-06f92ea8aeef>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mfaces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myw1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0myw2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxw1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mxw2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetected\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1783\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1785\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "\n",
    "def main():\n",
    "    depth = 16\n",
    "    k = 8\n",
    "    weight_file = \"weights.18-4.06.hdf5\"\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img_size = 64\n",
    "    model = WideResNet(img_size, depth=depth, k=k)()\n",
    "    model.load_weights(weight_file)\n",
    "\n",
    "    img = cv2.imread('Testing/trump.jpg')\n",
    "\n",
    "    input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "    detected = detector(input_img, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    #print(faces.shape)\n",
    "    for i, d in enumerate(detected):\n",
    "        x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "        xw1 = max(int(x1 - 0.4 * w), 0)\n",
    "        yw1 = max(int(y1 - 0.4 * h), 0)\n",
    "        xw2 = min(int(x2 + 0.4 * w), img_w - 1)\n",
    "        yw2 = min(int(y2 + 0.4 * h), img_h - 1)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        faces[i,:,:,:] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "        results = model.predict(np.expand_dims(faces[i],axis=0))\n",
    "        #print((np.expand_dims(faces[i],axis=0).shape))\n",
    "        #plt.imshow(faces)\n",
    "        predicted_genders = results[0]\n",
    "        #print(predicted_genders)\n",
    "        ages=np.zeros(101)\n",
    "        for j in range(101):\n",
    "            ages[j]=j\n",
    "        top3=(sorted(zip(results[1][0],ages), reverse=True)[:3])\n",
    "        print (top3)\n",
    "        answer_age=top3[0][1]\n",
    "        label = \"{}, {}\".format(int(answer_age),\n",
    "                                \"F\" if predicted_genders[0][0] > 0.5 else \"M\")\n",
    "        draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "    \n",
    "    plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WideResNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-6439636e6522>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mimg_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWideResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WideResNet' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"This script detects faces from web cam input, \"\n",
    "                                                 \"and estimates age and gender for the detected faces.\",\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--weight_file\", type=str, default=None,\n",
    "                        help=\"path to weight file (e.g. weights.18-4.06.hdf5)\")\n",
    "    parser.add_argument(\"--depth\", type=int, default=16,\n",
    "                        help=\"depth of network\")\n",
    "    parser.add_argument(\"--width\", type=int, default=8,\n",
    "                        help=\"width of network\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #args = get_args()\n",
    "    depth = 16\n",
    "    k = 8\n",
    "    weight_file = \"weights.18-4.06.hdf5\"\n",
    "\n",
    "    if not weight_file:\n",
    "        weight_file = \"weights.18-4.06.hdf5\"\n",
    "\n",
    "    # for face detection\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # load model and weights\n",
    "    img_size = 64\n",
    "    model = WideResNet(img_size, depth=depth, k=k)()\n",
    "    model.load_weights(weight_file)\n",
    "\n",
    "    # capture video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    while True:\n",
    "        # get video frame\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"error: failed to capture image\")\n",
    "            return -1\n",
    "\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "        # detect faces using dlib detector\n",
    "        detected = detector(input_img, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        #print(faces.shape)\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - 0.4 * w), 0)\n",
    "            yw1 = max(int(y1 - 0.4 * h), 0)\n",
    "            xw2 = min(int(x2 + 0.4 * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + 0.4 * h), img_h - 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "            faces[i,:,:,:] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "        #print(len(detected),faces.shape)\n",
    "        if len(detected) > 0:\n",
    "            # predict ages and genders of the detected faces\n",
    "            results = model.predict(faces)\n",
    "            #print(results[0])\n",
    "            ages=np.zeros(101)\n",
    "            for i in range(101):\n",
    "                ages[i]=i\n",
    "            top3=(sorted(zip(results[1][0],ages), reverse=True)[:3])\n",
    "            #print (top3)\n",
    "            answer_age=top3[0][1]\n",
    "            predicted_genders = results[0]\n",
    "            \n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}\".format(int(answer_age),\n",
    "                                    \"F\" if predicted_genders[i][0] > 0.5 else \"M\")\n",
    "            draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"result\", img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a993985e113e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# detect faces using dlib detector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mdetected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;31m#print(faces.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
