{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit-shah/Apparent-Age-Prediction-using-CNN/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BToPi2JX6jzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data and Glove Model From Drive"
      ]
    },
    {
      "metadata": {
        "id": "JvS3pf0t6zsk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8cf924f-29ad-429e-f2e1-403f8dce6d8d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e9s7l5uI7QWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "1e5873a8-20ce-4674-8935-fff98a614ae2"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction import text \n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk.corpus\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8c7Ay-J7Qwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp gdrive/'My Drive'/bbc-fulltext.zip .\n",
        "!cp gdrive/'My Drive'/glove.6B.zip .\n",
        "!unzip bbc-fulltext.zip > out.txt\n",
        "!rm bbc/README.TXT\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ew62XD1v790p",
        "colab_type": "code",
        "outputId": "af28a63c-aa70-476a-b86f-532525392f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "cell_type": "code",
      "source": [
        "#Read Data\n",
        "categories = ['business','entertainment','politics','sport','tech']\n",
        "df = pd.DataFrame([],columns=['category','id','text'])\n",
        "for cat in categories:\n",
        "  for file in os.listdir(\"bbc/\"+cat):\n",
        "      if file.endswith(\".txt\"):\n",
        "          filepath = os.path.join(\"bbc/\"+cat, file)\n",
        "          text = open(filepath,'r', errors='ignore').read()\n",
        "          s = pd.Series([cat,int(filepath.split('/')[-1][:-4]),text],index=['category','id','text'])\n",
        "          df = df.append(s,ignore_index=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>386</td>\n",
              "      <td>Krispy Kreme shares hit\\n\\nShares in Krispy Kr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>190</td>\n",
              "      <td>Consumers drive French economy\\n\\nFrance's eco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>439</td>\n",
              "      <td>German economy rebounds\\n\\nGermany's economy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>454</td>\n",
              "      <td>LSE doubts boost bidders' shares\\n\\nShares in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>302</td>\n",
              "      <td>Cactus diet deal for Phytopharm\\n\\nA slimming ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category   id                                               text\n",
              "0  business  386  Krispy Kreme shares hit\\n\\nShares in Krispy Kr...\n",
              "1  business  190  Consumers drive French economy\\n\\nFrance's eco...\n",
              "2  business  439  German economy rebounds\\n\\nGermany's economy, ...\n",
              "3  business  454  LSE doubts boost bidders' shares\\n\\nShares in ...\n",
              "4  business  302  Cactus diet deal for Phytopharm\\n\\nA slimming ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "uZXIyNwn8tT3",
        "colab_type": "code",
        "outputId": "a459248a-5f9c-476f-fd07-1069f9ea6349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#WordCloud\n",
        "cat = df['category'].unique()\n",
        "\n",
        "for i in range(0,len(cat)):\n",
        "    words = ' '.join(df.loc[df['category']==cat[i], 'text'])\n",
        "\n",
        "    wordcloud = WordCloud( \n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='white',\n",
        "                          width=800,\n",
        "                          height=400\n",
        "                ).generate(words)\n",
        "    print(cat[i])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "words = ' '.join(df.loc[:, 'text'])\n",
        "\n",
        "wordcloud = WordCloud( \n",
        "                      stopwords=STOPWORDS,\n",
        "                      background_color='white',\n",
        "                      width=800,\n",
        "                      height=400\n",
        "            ).generate(words)\n",
        "print(\"ALL Categories:\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#WordCloud\\ncat = df[\\'category\\'].unique()\\n\\nfor i in range(0,len(cat)):\\n    words = \\' \\'.join(df.loc[df[\\'category\\']==cat[i], \\'text\\'])\\n\\n    wordcloud = WordCloud( \\n                          stopwords=STOPWORDS,\\n                          background_color=\\'white\\',\\n                          width=800,\\n                          height=400\\n                ).generate(words)\\n    print(cat[i])\\n    plt.figure(figsize=(10, 5))\\n    plt.imshow(wordcloud)\\n    plt.axis(\\'off\\')\\n    plt.show()\\n\\nwords = \\' \\'.join(df.loc[:, \\'text\\'])\\n\\nwordcloud = WordCloud( \\n                      stopwords=STOPWORDS,\\n                      background_color=\\'white\\',\\n                      width=800,\\n                      height=400\\n            ).generate(words)\\nprint(\"ALL Categories:\")\\nplt.figure(figsize=(10, 5))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "JR2fsHlbr_Tj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Label Encoder\n",
        "#### Use inverse_transform at the end after predicting"
      ]
    },
    {
      "metadata": {
        "id": "385fGoXNldB0",
        "colab_type": "code",
        "outputId": "cac8f4ef-2190-4e28-bf18-0b537db95d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "y = df['category']\n",
        "X = df.drop(['category','id'],axis=1)\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X,y,stratify=y, test_size=0.2, random_state=123)\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "print('before: %s ...' %y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "print('after: %s ...' %y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: 1508       sport\n",
            "1908        tech\n",
            "2104        tech\n",
            "2019        tech\n",
            "1245    politics\n",
            "Name: category, dtype: object ...\n",
            "after: [3 4 4 ... 1 0 2] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n9jqvQtYDhR0",
        "colab_type": "code",
        "outputId": "6f0084e2-be0e-49ac-9876-cad5178f7cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.bincount(y_train))\n",
        "print(y.value_counts())\n",
        "print(le.inverse_transform([i for i in range(5)]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[408 309 333 409 321]\n",
            "sport            511\n",
            "business         510\n",
            "politics         417\n",
            "tech             401\n",
            "entertainment    386\n",
            "Name: category, dtype: int64\n",
            "['business' 'entertainment' 'politics' 'sport' 'tech']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1e6g_BjOsZ3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data - (split by \\n, remove periods, remove slashes)"
      ]
    },
    {
      "metadata": {
        "id": "_3Iv6MC3s26i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(X):\n",
        "  return X.map(lambda x: x.lower().split(\"\\n\")).map(lambda x: [y.split(\". \") for y in x]).map(lambda x: [i.replace('\\'','') for sl in x for i in sl if i is not ''])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6EgQMq_CtK8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preprocess both train and test separately\n",
        "X_train['text'] = preprocess(X_train['text'])\n",
        "X_test['text'] = preprocess(X_test['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wme2RkYu2UR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Stop Words and Lemmatization   \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopw = nltk.corpus.stopwords.words('english')\n",
        "punct = string.punctuation\n",
        "punct = word_tokenize(punct)\n",
        "punct += ['.','``','...','\\'s','--','-','n\\'t','\\'']\n",
        "stopw += punct\n",
        "def token_stop(text):\n",
        "    global stopw\n",
        "    global lemmatizer\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [lemmatizer.lemmatize(w) for w in words if not w in stopw]\n",
        "    return filtered\n",
        "  \n",
        "X_train['text'] = X_train['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])\n",
        "X_test['text'] = X_test['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHKBz-DG8HTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts = np.array(X_train['text'])\n",
        "test_text = np.array(X_test['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLe602IIEqV6",
        "colab_type": "code",
        "outputId": "a4ec4fc8-4200-4688-c495-efe526390ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9thZOjdY_Sl6",
        "colab_type": "code",
        "outputId": "991f0b8f-0b2f-4c89-af7c-a433cbf5410a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = to_categorical(np.asarray(y_train))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "np.random.seed(123)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "xtrain = data[:-nb_validation_samples]\n",
        "ytrain = labels[:-nb_validation_samples]\n",
        "xval = data[-nb_validation_samples:]\n",
        "yval = labels[-nb_validation_samples:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 28841 unique tokens.\n",
            "Shape of data tensor: (1780, 1000)\n",
            "Shape of label tensor: (1780, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VofHdiAqIRqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create glove embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3yooq2DI-dT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Keras Glove Embedding layer\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuZyVOsKKD6T",
        "colab_type": "code",
        "outputId": "2b3edd6a-c1cc-4d21-99e0-ba8628e5c915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Map label name to its index\n",
        "labels_index = {}\n",
        "for i in range(5):\n",
        "  name = le.inverse_transform([i])[0]\n",
        "  labels_index[name] = i\n",
        "labels_index"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "e18TBVrC9iwT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ]
    },
    {
      "metadata": {
        "id": "ijGeDWg6-XsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = \"CNN\"\n",
        "checkpointer = ModelCheckpoint(model_name + \"/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor=dice_coef, verbose=1,\n",
        "                               save_best_only=True, mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53s9TZ_mI_XY",
        "colab_type": "code",
        "outputId": "9daf8478-271a-4511-cc29-0acf8e2253e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(35)(x)  # global max pooling\n",
        "x = Dropout(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(len(labels_index), activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "model.fit(xtrain, ytrain, validation_data=(xval, yval),\n",
        "          epochs=10, batch_size=128)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 1424 samples, validate on 356 samples\n",
            "Epoch 1/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 1.8803 - acc: 0.3237 - val_loss: 1.1582 - val_acc: 0.7163\n",
            "Epoch 2/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.8361 - acc: 0.7149 - val_loss: 0.8825 - val_acc: 0.6292\n",
            "Epoch 3/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.4239 - acc: 0.8504 - val_loss: 0.3008 - val_acc: 0.8933\n",
            "Epoch 4/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.2609 - acc: 0.9199 - val_loss: 0.1441 - val_acc: 0.9607\n",
            "Epoch 5/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.3078 - acc: 0.8855 - val_loss: 0.1370 - val_acc: 0.9663\n",
            "Epoch 6/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.1118 - acc: 0.9635 - val_loss: 0.2129 - val_acc: 0.9270\n",
            "Epoch 7/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.1545 - acc: 0.9494 - val_loss: 0.1056 - val_acc: 0.9635\n",
            "Epoch 8/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.1278 - acc: 0.9529 - val_loss: 0.1291 - val_acc: 0.9635\n",
            "Epoch 9/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.1187 - acc: 0.9635 - val_loss: 0.1034 - val_acc: 0.9635\n",
            "Epoch 10/10\n",
            "1424/1424 [==============================] - 16s 11ms/step - loss: 0.0487 - acc: 0.9867 - val_loss: 0.1120 - val_acc: 0.9663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efe20f79940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "7BAwr4x19nMo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict on Test Data"
      ]
    },
    {
      "metadata": {
        "id": "1sM8QeKfJgRr",
        "colab_type": "code",
        "outputId": "f5b0348a-8e99-489b-a12a-d7f5ee05aa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "cell_type": "code",
      "source": [
        "#Tokenize test data and apply model prediction\n",
        "sequences = tokenizer.texts_to_sequences(test_text)\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "prediction = model.predict(data)\n",
        "prediction"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (445, 1000)\n",
            "Shape of label tensor: (1780, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.8750800e-01, 9.3460781e-04, 9.7416285e-03, 3.0001748e-04,\n",
              "        1.5157593e-03],\n",
              "       [9.9216952e-05, 5.7477209e-05, 9.9980348e-01, 2.2074584e-05,\n",
              "        1.7770984e-05],\n",
              "       [8.0846430e-10, 9.9999988e-01, 1.4362178e-07, 5.0976325e-08,\n",
              "        2.2003151e-08],\n",
              "       ...,\n",
              "       [1.7606765e-02, 4.1008536e-03, 9.7416592e-01, 2.0956560e-03,\n",
              "        2.0307389e-03],\n",
              "       [9.9922597e-01, 3.4078555e-05, 6.0908252e-04, 1.3478503e-05,\n",
              "        1.1742350e-04],\n",
              "       [3.4196011e-03, 6.6537422e-04, 9.9532300e-01, 2.1722028e-04,\n",
              "        3.7482221e-04]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "u_z5eMImMj57",
        "colab_type": "code",
        "outputId": "65ff8d54-95f9-44fa-f0c4-7ba6c0bcc2fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "t = 0\n",
        "for pred in prediction:\n",
        "  p = pred.argmax()\n",
        "  name = le.inverse_transform([p])\n",
        "  if name == y_test.loc[count]:\n",
        "    t+=1\n",
        "  count+=1\n",
        "    \n",
        "print('Test Accuracy:',t/count)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9820224719101124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZG11nLJFJ0UB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADFpnkxANf32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}